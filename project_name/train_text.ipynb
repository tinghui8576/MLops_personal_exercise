{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "# def train(config: str, wandbkey: Optional[str] = None, debug_mode: bool = False):\n",
    "from models.model import Model\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "batch = 8\n",
    "print_every = 50\n",
    "prefix = \"Content:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wikiData(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.labels = []\n",
    "        inputs = [prefix + \" \".join(str(text).split()[:10]) for text in df[\"body_text\"]]\n",
    "        input_tokenize = tokenizer( \n",
    "                                inputs,\n",
    "                                add_special_tokens=True,        #Add Special tokens like [CLS] and [SEP]\n",
    "                                max_length=max_length,\n",
    "                                padding = 'max_length',         #for padding to max_length for equal sequence length\n",
    "                                truncation = True,              #truncate the text if it is greater than max_length\n",
    "                                return_attention_mask=True,     #will return attention mask\n",
    "                                return_tensors=\"pt\"             #return tensor formate\n",
    "                                )\n",
    "\n",
    "        self.input_ids = torch.tensor(input_tokenize['input_ids'])\n",
    "        self.attention_mask = torch.tensor(input_tokenize['attention_mask'])\n",
    "        \n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            label_tokenize = tokenizer(\n",
    "                                    list(df[\"body_text\"]), \n",
    "                                    add_special_tokens=True,        #Add Special tokens like [CLS] and [SEP]\n",
    "                                    max_length=max_length,\n",
    "                                    padding = 'max_length',         #for padding to max_length for equal sequence length\n",
    "                                    truncation = True,              #truncate the text if it is greater than max_length\n",
    "                                    return_attention_mask=True,     #will return attention mask\n",
    "                                    return_tensors=\"pt\"\n",
    "                                    )\n",
    "                \n",
    "            self.labels = torch.tensor(label_tokenize['input_ids'])\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def valid(model, valid_dataloader,tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    for batch in valid_dataloader :\n",
    "        input_ids = batch[0]\n",
    "        masks = batch[1]\n",
    "        labels = batch[2]\n",
    "        # Turn off gradients for validation, will speed up inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    random_batch = random.choice(list(valid_dataloader))\n",
    "\n",
    "    \n",
    "    original_text = tokenizer.decode(random_batch[2][0], skip_special_tokens=True) \n",
    "    print(\"Original content:\", original_text)\n",
    "    outputs = model.generate(random_batch[0])\n",
    "    print(\"Generate content:\",outputs)\n",
    "\n",
    "    return(running_loss/len(valid_dataloader))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Hyperparameters\n",
    "    lr = 5e-5\n",
    "    epochs = 20\n",
    "    batch = 8\n",
    "    print_every = 50\n",
    "    seed = 123\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    model = Model(lr)\n",
    "\n",
    "    # Optimizer and tokenizer \n",
    "    tokenizer = model.tokenizer\n",
    "    optimizer = model.configure_optimizers()\n",
    "\n",
    "    \n",
    "    # Readfile and make to dataloader\n",
    "    filepath = \"../data/processed/\"\n",
    "    df_train = pd.read_csv(filepath+'train.csv')\n",
    "    df_valid = pd.read_csv(filepath+'valid.csv')    \n",
    "    train_data = wikiData(df_train.head(2000) ,tokenizer,max_target_length)\n",
    "    valid_data = wikiData(df_valid.head(200) ,tokenizer,max_target_length)\n",
    "    train_dataloader = DataLoader(train_data, batch_size =batch,shuffle= True)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size =batch,shuffle= True)\n",
    "\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        print(\"Epoch: {}/{}.. \".format(e + 1, epochs))\n",
    "        for steps, batch in enumerate(train_dataloader):\n",
    "            # load data and labels in the batch\n",
    "            input_ids = batch[0]\n",
    "            masks = batch[1]\n",
    "            labels = batch[2]\n",
    "\n",
    "            # Training\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            if steps % print_every == 0 and not steps == 0:\n",
    "                # original_text = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "                # print(\"Original content:\", original_text)\n",
    "                # outputs = model.generate(input_ids)\n",
    "                # print(\"Generate content:\",outputs)\n",
    "                print(\n",
    "                    \"Batch: {}/{}.. \".format(steps, len(train_dataloader)),\n",
    "                    \"Training Loss: {:.3f}.. \".format(running_loss / print_every))\n",
    "                running_loss = 0\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        valid_loss = valid(model, valid_dataloader, tokenizer)\n",
    "        \n",
    "        print(\n",
    "            \"Training Loss: {:.3f}.. \".format(train_loss / len(train_dataloader)),\n",
    "            \"Valid Loss: {:.3f} \".format(valid_loss),)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinghui/workspace/github/DTU02476-Machine-Learning-Operations/env/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/tinghui/workspace/github/DTU02476-Machine-Learning-Operations/env/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/var/folders/z2/m5_kp65x4ydbtt3mq8bcpw_r0000gn/T/ipykernel_55463/3603335207.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.input_ids = torch.tensor(input_tokenize['input_ids'])\n",
      "/var/folders/z2/m5_kp65x4ydbtt3mq8bcpw_r0000gn/T/ipykernel_55463/3603335207.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.attention_mask = torch.tensor(input_tokenize['attention_mask'])\n",
      "/Users/tinghui/workspace/github/DTU02476-Machine-Learning-Operations/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/var/folders/z2/m5_kp65x4ydbtt3mq8bcpw_r0000gn/T/ipykernel_55463/3603335207.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(label_tokenize['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20.. \n",
      "Batch: 50/250..  Training Loss: 6.037.. \n",
      "Batch: 100/250..  Training Loss: 4.562.. \n",
      "Batch: 150/250..  Training Loss: 4.275.. \n",
      "Batch: 200/250..  Training Loss: 4.302.. \n",
      "Original content: Contents1 Directions2 Content3 Martin Seligman4 Learned Helplessness5 Research6 Imprinting7 Essay Topics8 BibliographyDirectionsedit edit sourceThis content should include the following itemsImprinting and Learned helplessness Lorenz and SeligmanContentedit edit sourceMartin Seligmanedit edit sourceThrough Martin Seligmans experiments and the evaluation and observation of human behavior over the past decades by other researchers in his field psychologists have solidified the theory of passive resignation better known as learned helplessness The identification of Learned Helplessness has led to a greater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinghui/workspace/github/DTU02476-Machine-Learning-Operations/env/lib/python3.11/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate content: \n",
      "Training Loss: 4.648..  Valid Loss: 3.695 \n",
      "Epoch: 2/20.. \n",
      "Batch: 50/250..  Training Loss: 4.198.. \n",
      "Batch: 100/250..  Training Loss: 3.916.. \n",
      "Batch: 150/250..  Training Loss: 3.955.. \n",
      "Batch: 200/250..  Training Loss: 3.832.. \n",
      "Original content: Specialsearchpetroleum prefixEnglishHanziSpecialsearchoil prefixEnglishHanziPetroleum is an oil that is under the ground or the sea It is used to produce petrol kerosene diesel etc\n",
      "Generate content: \n",
      "Training Loss: 3.936..  Valid Loss: 3.527 \n",
      "Epoch: 3/20.. \n",
      "Batch: 50/250..  Training Loss: 3.848.. \n",
      "Batch: 100/250..  Training Loss: 3.739.. \n",
      "Batch: 150/250..  Training Loss: 3.711.. \n",
      "Batch: 200/250..  Training Loss: 3.695.. \n",
      "Original content: Picture Function or Picture Statementedit edit sourceWhich of the following did you need help with Please click a link to go to the desired pagePicture FunctionpictureHandle a hrefpicture20functionhtmlPICTUREaPicture Statementa hrefpicture20statementhtmlPICTUREa h1v1h2v2 pictureHandle\n",
      "Generate content: \n",
      "Training Loss: 3.733..  Valid Loss: 3.417 \n",
      "Epoch: 4/20.. \n",
      "Batch: 50/250..  Training Loss: 3.702.. \n",
      "Batch: 100/250..  Training Loss: 3.589.. \n",
      "Batch: 150/250..  Training Loss: 3.588.. \n",
      "Batch: 200/250..  Training Loss: 3.640.. \n",
      "Original content: Please share your thoughts about whether to keep this page at WikibooksMaintain this notice so this page can receive a fair review and people have time to boldly address reasonable concerns through concrete improvements Remove or replace this notice after discussion concludes and a community decision is made Please also consider notifying the primary contributors on their discussion page withsubstRfd warningPinyinBus fire kills 14 in Ningxia 20160105 Search busSearch fireNotePinyinPinyin tone markingA fire has attacked a bus in the province of Ningxia\n",
      "Generate content: Please share your thoughts about whether to keep this page in the future.\n",
      "Training Loss: 3.612..  Valid Loss: 3.363 \n",
      "Epoch: 5/20.. \n",
      "Batch: 50/250..  Training Loss: 3.634.. \n",
      "Batch: 100/250..  Training Loss: 3.540.. \n",
      "Batch: 150/250..  Training Loss: 3.410.. \n",
      "Batch: 200/250..  Training Loss: 3.630.. \n",
      "Original content: z test statistic one proportionedit edit sourceA test statistic that represents the difference between the hypothesized proportion and the observed proportion is used when the binomial parameter p is being tested through the use of a hypothesistesting procedure This value is then divided by the standard error When n is significantly large n 20 np 5 and nq 5 the test assumptions have been satisfied and the null hypothesis is true then we assume that the test statistic is normally distributed\n",
      "Generate content: Test statistic one proportionedit edit sourceA test statistic that is a test statistic that is\n",
      "Training Loss: 3.543..  Valid Loss: 3.337 \n",
      "Epoch: 6/20.. \n",
      "Batch: 50/250..  Training Loss: 3.527.. \n",
      "Batch: 100/250..  Training Loss: 3.515.. \n",
      "Batch: 150/250..  Training Loss: 3.623.. \n",
      "Batch: 200/250..  Training Loss: 3.424.. \n",
      "Original content: NumberNumber SystemTypes of NumberNumber OperationVariableVariableExpressionPolynomialFunctionFunction DefinitionLinear FunctionNon Linear FunctionEquationPolynominal EquationDifferential Equation\n",
      "Generate content: NumberNumber SystemTypes of NumberNumber OperationVariable\n",
      "Training Loss: 3.491..  Valid Loss: 3.303 \n",
      "Epoch: 7/20.. \n",
      "Batch: 50/250..  Training Loss: 3.532.. \n",
      "Batch: 100/250..  Training Loss: 3.526.. \n",
      "Batch: 150/250..  Training Loss: 3.371.. \n",
      "Batch: 200/250..  Training Loss: 3.445.. \n",
      "Original content: Contents1 Babylon the Great11 Verses 1212 Verse 313 Verses 4514 Verse 62 The Angel Explains the Vision21 Verses 7822 Verse 923 Verse 1024 Verse 1125 Verses 121326 Verse 1427 Verse 1528 Verses 161729 Verse 18Babylon the Greatedit edit sourceVerses 12edit edit source1One of the seven angels who had the seven bowls came and talked with me saying to me Come here I will show you the judgment of the great whore that sits upon many waters 2with whom the\n",
      "Generate content: Contents1 Babylon the Great11 Verses 1212 Verse 313 Verses 4514\n",
      "Training Loss: 3.449..  Valid Loss: 3.285 \n",
      "Epoch: 8/20.. \n",
      "Batch: 50/250..  Training Loss: 3.473.. \n",
      "Batch: 100/250..  Training Loss: 3.434.. \n",
      "Batch: 150/250..  Training Loss: 3.383.. \n",
      "Batch: 200/250..  Training Loss: 3.367.. \n",
      "Original content: MINC Frequently asked questions will be hereedit edit sourceWhere to ask for helpQuestions on using MINC MINCusers mailing list MINCusersbicmnimcgillca httpwwwbicmnimcgillcamailmanlistinfomincusersQuestions on MINC development MINCdevelopment mailing list MINCdevelopmentbicmnimcgillca httpwwwbicmnimcgillcamailmanlistinfomincdevelopment\n",
      "Generate content: MINC Frequently asked questions will be hereedit edit sourceWhere to find a solution to\n",
      "Training Loss: 3.414..  Valid Loss: 3.267 \n",
      "Epoch: 9/20.. \n",
      "Batch: 50/250..  Training Loss: 3.489.. \n",
      "Batch: 100/250..  Training Loss: 3.398.. \n",
      "Batch: 150/250..  Training Loss: 3.463.. \n",
      "Batch: 200/250..  Training Loss: 3.326.. \n",
      "Original content: Contents1 Lets go11 Installation of the core111 Enabling preferences112 Validating portal rules12 Adding packages121 General Idea122 erp5base1221 Basic use case12211 Step 1 create an Organisation12212 Step 2 create a Person12213 Step 3 employ the Person12214 Step 4 make the person a user123 erp5trade1231 Basic use case12311 Scenario description12312 System preparation12313 Purchasing goods12314 and selling themLets goedit edit sourceInstallation of the coreedit edit sourceAn alternative way of installing and running\n",
      "Generate content: Contents1 Lets go11 Installation of the core111 Enabling preferences112 Valid\n",
      "Training Loss: 3.378..  Valid Loss: 3.253 \n",
      "Epoch: 10/20.. \n",
      "Batch: 50/250..  Training Loss: 3.478.. \n",
      "Batch: 100/250..  Training Loss: 3.462.. \n",
      "Batch: 150/250..  Training Loss: 3.262.. \n",
      "Batch: 200/250..  Training Loss: 3.351.. \n",
      "Original content: Chapter 2 of Harry Potter and the Philosophers Stone The Vanishing Glass Chapter 1 Chapter 3 Contents1 Synopsis2 Analysis3 Questions31 Review32 Further Study4 Greater Picture41 ConnectionsSynopsisedit edit sourceSpoiler warning Plot andor ending details followAlmost ten years have passed since baby Harry now nearly 11 years old was left with the Dursleys Harry has grown into a skinny boy with unruly black hair and green eyes hidden behind round glasses He also has a lightningboltshaped scar on his forehead which his aunt and uncle attribute to the supposed car crash\n",
      "Generate content: Chapter 2 of Harry Potter and the Philosophers Stone The Philosophers Stone The Philosophers Stone The\n",
      "Training Loss: 3.353..  Valid Loss: 3.240 \n",
      "Epoch: 11/20.. \n",
      "Batch: 50/250..  Training Loss: 3.392.. \n",
      "Batch: 100/250..  Training Loss: 3.310.. \n",
      "Batch: 150/250..  Training Loss: 3.329.. \n",
      "Batch: 200/250..  Training Loss: 3.304.. \n",
      "Original content: Motivationedit edit sourceYou want to limit search results based on one or more dates within itemsMethodedit edit sourceWe will create a XForms application that will have two calendar selectors We can use the builtin calendar selector or allow users to select the year month day values separatelyMonth Code Tableedit edit sourcecodetable codetablenamedatemonthcodecodetablename descriptionA list of all the month codes for doing searchsdescription items item labelJanuarylabel numericid1numericid abbreviationJanabbreviation valuejanuaryvalue\n",
      "Generate content: Motivationedit edit sourceYou want to limit search results based on search results based\n",
      "Training Loss: 3.323..  Valid Loss: 3.229 \n",
      "Epoch: 12/20.. \n",
      "Batch: 50/250..  Training Loss: 3.221.. \n",
      "Batch: 100/250..  Training Loss: 3.411.. \n",
      "Batch: 150/250..  Training Loss: 3.243.. \n",
      "Batch: 200/250..  Training Loss: 3.309.. \n",
      "Original content: Contents1 Motivation2 Screen Image3 Link to XForms Application4 Sample Program5 Discussion6 ReferencesMotivationedit edit sourceIn some instances you only want to add to the end of a list or delete from the end of a list This program demonstrates how you can use the last function to do thisScreen Imageedit edit source Screen ImageWhen you insert a new row it will always be inserted at the end of the list When you delete a row it will also be removed from the end of the listLink to XForms Applicationedit edit sourceIns\n",
      "Generate content: Contents1 Motivation2 Screen Image3 Link to XForms Application4 Sample Program\n",
      "Training Loss: 3.297..  Valid Loss: 3.220 \n",
      "Epoch: 13/20.. \n",
      "Batch: 50/250..  Training Loss: 3.347.. \n",
      "Batch: 100/250..  Training Loss: 3.229.. \n",
      "Batch: 150/250..  Training Loss: 3.396.. \n",
      "Batch: 200/250..  Training Loss: 3.174.. \n",
      "Original content: QuadrilateralsInterior AnglesExterior AnglesClassifying QuadrilateralsUsing ParallelogramsProving Quadrilaterals are ParallelogramsRhombi Rectangles and SquaresTrapezoidsKites\n",
      "Generate content: QuadrilateralsInterior AnglesExterior AnglesClassifying\n",
      "Training Loss: 3.276..  Valid Loss: 3.213 \n",
      "Epoch: 14/20.. \n",
      "Batch: 50/250..  Training Loss: 3.213.. \n",
      "Batch: 100/250..  Training Loss: 3.315.. \n",
      "Batch: 150/250..  Training Loss: 3.351.. \n",
      "Batch: 200/250..  Training Loss: 3.325.. \n",
      "Original content: This page explains how to translate an Interlingual Energizer Just follow these stepsCreate an account Its much easier to attribute your translations and to make them part of the book when you are logged in Registering is very easy and doesnt even require an email addressChoose an energizer Choose a description which you would like to translate Have a look at the full list of energizersOpen the page by clicking on the link in the list You will get for example to SamuraiStart editing At the top of the page is a button which says Edit Press\n",
      "Generate content: This page explains how to translate an Interlingual Energizer Just to the Energizer\n",
      "Training Loss: 3.257..  Valid Loss: 3.202 \n",
      "Epoch: 15/20.. \n",
      "Batch: 50/250..  Training Loss: 3.316.. \n",
      "Batch: 100/250..  Training Loss: 3.210.. \n",
      "Batch: 150/250..  Training Loss: 3.128.. \n",
      "Batch: 200/250..  Training Loss: 3.259.. \n",
      "Original content: Numerical MethodsOften we need to find the integral of a function that may be difficult to integrate analytically ie as a definite integral or impossible the function only existing as a table of valuesSome methods of approximating said integral are listed belowContents1 Trapezoidal Rule11 Example12 Error Analysis2 Simpsons Rule21 Example22 Error Analysis3 Simpsons 384 Headline text41 Example42 Error Analysis5 References and further readingTrapezoidal Ruleedit edit sourceConsider some function possibly unknown f x displaystyle\n",
      "Generate content: Numerical MethodsOften we need to find the integral of a Numerical\n",
      "Training Loss: 3.226..  Valid Loss: 3.195 \n",
      "Epoch: 16/20.. \n",
      "Batch: 50/250..  Training Loss: 3.261.. \n",
      "Batch: 100/250..  Training Loss: 3.271.. \n",
      "Batch: 150/250..  Training Loss: 3.181.. \n",
      "Batch: 200/250..  Training Loss: 3.100.. \n",
      "Original content: Energy Couplingedit edit source The total G is negative because of the coupling of reactions In this diagram G1 stands for the change in G resulting from the reaction of glucose Pi Glucose 6phosphate G2 is the free energy resulting from the reaction of ATP ADP Pi and G3 is the total change in free energy by coupling these two reactions togetherMany chemicals reactions are not spontaneous and require energy to occur The spontaneity of a chemical reaction is determined by its Gibbs free energy value If negative the reaction will proceed spontaneously if positive the reaction will not be\n",
      "Generate content: Energy Couplingedit edit source The total G is negative because it is a s\n",
      "Training Loss: 3.206..  Valid Loss: 3.186 \n",
      "Epoch: 17/20.. \n",
      "Batch: 50/250..  Training Loss: 3.254.. \n",
      "Batch: 100/250..  Training Loss: 3.030.. \n",
      "Batch: 150/250..  Training Loss: 3.222.. \n",
      "Batch: 200/250..  Training Loss: 3.250.. \n",
      "Original content: Semasiological map for holkaR Blr Ukr Pol Cz holka Slk Sln holkaMeanings needle girl no such wordFor areas without shading there is no information yetholkaMeaningsneedlegirlR Blr Ukr Pol Cz holka Slk Sln Index of semasiological maps False Friends of the Slavist Homepage\n",
      "Generate content: Semasiological map for holkaR Blr Ukr Pol C\n",
      "Training Loss: 3.183..  Valid Loss: 3.181 \n",
      "Epoch: 18/20.. \n",
      "Batch: 50/250..  Training Loss: 3.194.. \n",
      "Batch: 100/250..  Training Loss: 3.198.. \n",
      "Batch: 150/250..  Training Loss: 3.168.. \n",
      "Batch: 200/250..  Training Loss: 3.213.. \n",
      "Original content: A sense of success John Osheredit edit sourceWhat do Hasbro Gerber and Proctor Gamble have in common They all bought businesses started by John Osher Toys baby products and the bestselling SpinBrush toothbrush are only some of the concepts that Osher has launched into successful ventures Hes been starting businesses since the age of eight and believes he possesses a unique worldviewjust as a comedian sees humor in the everyday Osher sees opportunity People have caught on and now Osher is surrounded by great ideasaspiring inventors send him their plans and he\n",
      "Generate content: An sense of success John Osheredit edit sourceWhat do Hasbro do with his work\n",
      "Training Loss: 3.165..  Valid Loss: 3.173 \n",
      "Epoch: 19/20.. \n",
      "Batch: 50/250..  Training Loss: 3.204.. \n",
      "Batch: 100/250..  Training Loss: 3.111.. \n",
      "Batch: 150/250..  Training Loss: 3.225.. \n",
      "Batch: 200/250..  Training Loss: 3.020.. \n",
      "Original content: Wireless Mesh Networks Acknowledgements Introduction Mesh network basics History Highlights Motivation Economics Politics Social Impact Regulatory Taxonomy of Mesh Types Design Parameters How many radios Transport Access RF concerns What layers does it run on Topology Control Mobility Handoffs Applications Content Operations Accounting and Billing Management Getting Power to the Node Survey Tools Planning Tools Mounting Options Testing Tools Meshes you can build Conclusion Applicable Standards Known Deployments Mesh Timeline Mesh Network Vendors Bibliography Glossary Index Contents1 Topology Control11 Loop\n",
      "Generate content: Wireless Mesh Networks Acknowledgements Introduction Mesh network basics History Highlights\n",
      "Training Loss: 3.145..  Valid Loss: 3.168 \n",
      "Epoch: 20/20.. \n",
      "Batch: 50/250..  Training Loss: 3.153.. \n",
      "Batch: 100/250..  Training Loss: 3.213.. \n",
      "Batch: 150/250..  Training Loss: 3.140.. \n",
      "Batch: 200/250..  Training Loss: 3.088.. \n",
      "Original content: Closed Gameabcdefgh8877665544332211abcdefghPosition in ForsythEdwards Notation FENrnbqkbnrppp1pppp83p43P48PPP1PPPPRNBQKBNR Moves 1d4 d5ECO code D00D69Contents1 Closed Game11 1d512 Theory table13 ReferencesClosed Gameedit edit source1d5edit edit sourceWith 1d5 Black begins to fight for the center in the traditional\n",
      "Generate content: Gameabcdefgh8877665544332211abc\n",
      "Training Loss: 3.123..  Valid Loss: 3.164 \n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
